[
  {
    "objectID": "Journal/josh.html",
    "href": "Journal/josh.html",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Machine learning in medicine: a practical introduction the authors provide a introduction to machine learning in the medical field and a survey of 3 supervised learning methodologies. The begin by explaining how machine learning is related to traditional statistical inference but noted that a major difference is that statistical inference aims to “reach conclusions about a population…” (Sidey-Gibbons, 2) while machine learning aims to predict a specific out come. The authors go on to state that the use of ML in the medical field is relatively easy to explain because many features or parameters that are input can be reasoned about when the prediction is made. An example the authors used were “body mass index and diabetes risk” as the linkage between the two are relatively well known and understood. The author’s then go on to explain the difference between Auditable Algorithms that are easily understood and black box algorithms which are complex and can be hard to reason about. Support Vector Machines can be a member of the later group but not always.\nThe authors then leverage a Generalized Linear model, Support Vector Machine, and Artificial Neural Network to predict whether or not a given breast tissue sample is cancerous. I will skip over the ANN and GLM as our group is not directly focused on them. However with the Support Vector Machine implementation they authors did note that the overall goal is to build a hyperplane that separates two categories the best. Some datasets do not easily separate so it is possible to rearrange the date by using a kernel trick or kernel function to increase the amount of linear separation between observations.\nFinally the authors compared the outcomes of their three algorithms and found that the best one (in terms of accuracy) for the data set they had was the support vector machine. The way they determined this was to build a ROC Curve to find the number of true true predictions and true false predictions\nUWF Access Url\n\n\n\nIn Dibiki et al’s paper on Support vector machines he discussed what SVM’s are and what that the underlying statistical methods are. The paper overall was heavily math based and at times over my head. However it did have information about how and why support vector machines are built. Predominately the author notes that reseacher Vapnik constructed SVM’s based off of Structural Risk Minimaization(SRM) as opposed to Empirical Risk minimization and that SRM proved to be better at creating more generalizable models.\nThe author goes on to explain that the hyperplane is a the maximal margin between data points in a data set. This is called the “Optimal Separating Hyperplane” The authors then went through the math of how to solve for the problems and examples of how to chose kernel functions to ensure that a hyperplane can be found.\nThe examples the author used was were the classification if image data to determine classification of land cover (water, forest, roads, etc) Another example was comparing the performance of an Artificial Neural Network to Support Vector machines to predict stream flow data based on daily rainfall and evaporation based on 3 different locations. The general finding was that SVM and ANN’s can both be readily leveraged to build similar models and predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#machine-learning-in-medicine-a-practical-introduction.-gibbons2019",
    "href": "Journal/josh.html#machine-learning-in-medicine-a-practical-introduction.-gibbons2019",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Machine learning in medicine: a practical introduction the authors provide a introduction to machine learning in the medical field and a survey of 3 supervised learning methodologies. The begin by explaining how machine learning is related to traditional statistical inference but noted that a major difference is that statistical inference aims to “reach conclusions about a population…” (Sidey-Gibbons, 2) while machine learning aims to predict a specific out come. The authors go on to state that the use of ML in the medical field is relatively easy to explain because many features or parameters that are input can be reasoned about when the prediction is made. An example the authors used were “body mass index and diabetes risk” as the linkage between the two are relatively well known and understood. The author’s then go on to explain the difference between Auditable Algorithms that are easily understood and black box algorithms which are complex and can be hard to reason about. Support Vector Machines can be a member of the later group but not always.\nThe authors then leverage a Generalized Linear model, Support Vector Machine, and Artificial Neural Network to predict whether or not a given breast tissue sample is cancerous. I will skip over the ANN and GLM as our group is not directly focused on them. However with the Support Vector Machine implementation they authors did note that the overall goal is to build a hyperplane that separates two categories the best. Some datasets do not easily separate so it is possible to rearrange the date by using a kernel trick or kernel function to increase the amount of linear separation between observations.\nFinally the authors compared the outcomes of their three algorithms and found that the best one (in terms of accuracy) for the data set they had was the support vector machine. The way they determined this was to build a ROC Curve to find the number of true true predictions and true false predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#model-induction-with-support-vector-machines-introductions-and-applications.dibike2001",
    "href": "Journal/josh.html#model-induction-with-support-vector-machines-introductions-and-applications.dibike2001",
    "title": "Josh’s Journal Entries",
    "section": "",
    "text": "In Dibiki et al’s paper on Support vector machines he discussed what SVM’s are and what that the underlying statistical methods are. The paper overall was heavily math based and at times over my head. However it did have information about how and why support vector machines are built. Predominately the author notes that reseacher Vapnik constructed SVM’s based off of Structural Risk Minimaization(SRM) as opposed to Empirical Risk minimization and that SRM proved to be better at creating more generalizable models.\nThe author goes on to explain that the hyperplane is a the maximal margin between data points in a data set. This is called the “Optimal Separating Hyperplane” The authors then went through the math of how to solve for the problems and examples of how to chose kernel functions to ensure that a hyperplane can be found.\nThe examples the author used was were the classification if image data to determine classification of land cover (water, forest, roads, etc) Another example was comparing the performance of an Artificial Neural Network to Support Vector machines to predict stream flow data based on daily rainfall and evaporation based on 3 different locations. The general finding was that SVM and ANN’s can both be readily leveraged to build similar models and predictions\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#data-mining-concepts-and-techniques-han2012",
    "href": "Journal/josh.html#data-mining-concepts-and-techniques-han2012",
    "title": "Josh’s Journal Entries",
    "section": "Data Mining: Concepts and Techniques (Han and Pei 2012)",
    "text": "Data Mining: Concepts and Techniques (Han and Pei 2012)\nThis review of the text book contains more general information about support vector machines and various applications. It defines that the maximal marginal hyperplane is the single line that can be drawn between two “clusters” of data.This MMH (maximal marginal hyperplane) is defined as a line that can be drawn where the distances between two clusters of data is the largest. It also explains that while we can think of it as a line, the MMH is an actual plane that can support more than 2 dimensions. Additionally the vectors where the hyperplane touch are called the support vectors as the effectively define the sides of the hyper plane.\nIn general the book explains that Support vector machines work very well with linear data sets, their use of kernel functions allow them to operate on non linear data. Kernel functions according to the book can be thought of as mathematic tricks that transform/map data from one dimension to a higher dimensions that is linearly separable."
  },
  {
    "objectID": "Journal/josh.html#support-vector-machines-and-kernel-methods-the-new-generation-of-learning-machines-cristianini2002",
    "href": "Journal/josh.html#support-vector-machines-and-kernel-methods-the-new-generation-of-learning-machines-cristianini2002",
    "title": "Josh’s Journal Entries",
    "section": "Support vector machines and kernel methods: the new generation of learning machines (Cristianini and Scholkopf Fall 2002)",
    "text": "Support vector machines and kernel methods: the new generation of learning machines (Cristianini and Scholkopf Fall 2002)\nIn the article, the author briefly discusses the history of machine learning and its evolution from working predominately on linearly separable datasets to the advancements made with handling non linear data in the 1980s. The author then goes on to discuss the work and presentations of Vapnik et al in 1992. This allowed those wanting to do machine learning data on non linear data in a “principled yet efficient manner”.  (Cristianini and Scholkopf)\none of the other key points that the other makes is that the higher the dimensionality of the problems space, the harder it is to create predictions based on it.\nThe other new and interesting points pointed by the authors where that SVM’s hold records (at the time the article was written) for ability to read handwritten digits and other tasks.This leads to it being very well suited for hand writing detection. Other areas that the models are good according to the author are “text categorization, handwritten digit recognition, and gene expression data classification” (Cristianini and Scholkopf)\nUWF Access Url"
  },
  {
    "objectID": "Journal/josh.html#fast-training-of-support-vector-machines-for-survival-analysis-polsterl2015",
    "href": "Journal/josh.html#fast-training-of-support-vector-machines-for-survival-analysis-polsterl2015",
    "title": "Josh’s Journal Entries",
    "section": "Fast Training of Support Vector Machines for Survival Analysis (Pölsterl, Navab, and Katouzian 2015)",
    "text": "Fast Training of Support Vector Machines for Survival Analysis (Pölsterl, Navab, and Katouzian 2015)\nIn Fast Training of Support Vector Machines for Survival Analysis the author explains that they wish to look at 3 different methods of training a support vector machine for survival analysis: ranking, regression, and a combination of ranking and regressions to determine how well they predict survivability. The author also introduces the concept of censored data. This is a term i hadn’t heard of before but makes sense when explained and in the context of survivability prediction. As explained by the author, data is uncensored if a significant event occurs during the time period in which the study or model is used for. Meanwhile censored data is data in which the event did not occur during the study or observation period, however it may have occurred after the study completes. When thinking about patient survivability this makes sense. For example if we want to study whether or not a patient dies during a hospital stay we probably only want to predict that for a fixed time period (the hospital stay) as we all know every patient will eventually die…perhaps even in a (different) hospital stay!\nThe author then goes on to show how ranking methods such as Cox proportional Hazards and others fair when given certain sized data sets and then comparing that with regression based model. The author summarizes at the end that using ordered statistic trees (which their algorithm uses) is a sufficiently accurate and fast model for predicting patient survivability."
  },
  {
    "objectID": "Journal/josh.html#mortality-prediction-based-on-imbalanced-high-dimensional-icu-big-data-liu2018",
    "href": "Journal/josh.html#mortality-prediction-based-on-imbalanced-high-dimensional-icu-big-data-liu2018",
    "title": "Josh’s Journal Entries",
    "section": "Mortality prediction based on imbalanced high-dimensional ICU big Data (Liu June 2018)",
    "text": "Mortality prediction based on imbalanced high-dimensional ICU big Data (Liu June 2018)\nMortality prediction based on imbalanced high-dimensional ICU big data takes a look at predicting mortality based on a large number of data dimensions with various amounts of data missing. Over all this paper appears to follow an approach that would be good for our project using the MIMIC data set.\nMost of the article goes beyond the scope of Support Vector Machines but delves into principal component analysis to determine what to use to build the support vector machines.The author leverages Cost Sensitive Principal Component Analysis to preprocess the data to deal with missing data and feature extraction. Once this preprocessing step has completed, the authors build a support vector machine to predict mortality. The also build a number of other support vector machines using Chaos particle swarm optimization for parameter optimization and derivatives of CPSO to determine the best model based on the ROC AUC value. In the end the found the SVM using data that had been processed with their modified Cost Sensitive Principal Component Analysis and SPSO\nThrough their findings the authors also opine about the large amount of data and the necessity of determining which are the key features to from which to build a model such as a SVM. They state that the overall number of data points will continue to increase as sensors and technology are continually introduced and improved upon in medical settings and that while the data is great and represents a virtual gold mine, it is important to ensure that data is clean and useful for prediction and not just noisy data for algorithms to churn through"
  },
  {
    "objectID": "Journal/josh.html#artificial-intelligence-in-the-intensive-care-unit-greco2020",
    "href": "Journal/josh.html#artificial-intelligence-in-the-intensive-care-unit-greco2020",
    "title": "Josh’s Journal Entries",
    "section": "Artificial Intelligence in the Intensive Care Unit (Greco, Caruso, and Cecconi 2020)",
    "text": "Artificial Intelligence in the Intensive Care Unit (Greco, Caruso, and Cecconi 2020)\nThe authors of Artificial Intelligence in the Intensive Care Unit describe the ways that medicine can benefit by the usage of machine learning and compares various methods for machine learning. The author goes in depth about why Intensive Care Units are a great place for the introduction of big data practices and machine learning in hospital settings. After introducing the methods of machine learning the paper then discusses the limits of machine learning, examples of machine learning in critical care and the future of big data and machine learning in medicine.\nLive Access URL"
  },
  {
    "objectID": "Journal/josh.html#predictive-modelling-of-survival-and-length-of-stay-in-critically-ill-patients-using-sequential-organ-failure-scores-houthooft2015",
    "href": "Journal/josh.html#predictive-modelling-of-survival-and-length-of-stay-in-critically-ill-patients-using-sequential-organ-failure-scores-houthooft2015",
    "title": "Josh’s Journal Entries",
    "section": "Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores (Houthooft et al. 2015)",
    "text": "Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores (Houthooft et al. 2015)\nThe paper “Predictive modelling of survival and length of stay in critically ill patients using sequential organ failure scores” the authors talk about ways to use machine learning to model the length of stay as a predictor of patient mortality. The author talks about choosing the data sets and selecting certain features for modeling based on data from th first five days of a patients stay to predict both the patients mortality and their length of stay. The results of the SVM the author built were compared to regression results to model the length of stay and then compared with mortality for patients with lengthy ICE stays. The author goes on to conclude that the models can be helpful to support physicians allocate ICU resources and make decisions during a patients time in ICU."
  },
  {
    "objectID": "Journal/eric.html",
    "href": "Journal/eric.html",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://www.jstatsoft.org/article/view/v015i09\nThe above article from the Journal of statistical Software outlines what Support Vector Machines are and what their use cases can be. The article continues by leveraging the mathematical equations for classification and regression as well as deployment strategies for data sets in R. The article concludes with examples of code and outputs showcasing the results on the iris data set.\n\n\n\nhttps://seis.bristol.ac.uk/~enicgc/pubs/1999/ijcai_ss.pdf\nThe above speaks to controlling the sensitivity of Support Vector Machines to reduce the number of False Positives/Negatives in the output. Veropoulos, Campbell, and Cristianini go on to detail the difference between Sensitivity and Specificity through various mathematical approaches and analyses. Their research with medical data sets where Box constraints were not statistically significant between the 4 data sets. However, the strain on the algorithm was insignificant and could aid in over all determinations."
  },
  {
    "objectID": "Journal/eric.html#support-vector-machines-in-r-karatzoglou2006",
    "href": "Journal/eric.html#support-vector-machines-in-r-karatzoglou2006",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://www.jstatsoft.org/article/view/v015i09\nThe above article from the Journal of statistical Software outlines what Support Vector Machines are and what their use cases can be. The article continues by leveraging the mathematical equations for classification and regression as well as deployment strategies for data sets in R. The article concludes with examples of code and outputs showcasing the results on the iris data set."
  },
  {
    "objectID": "Journal/eric.html#controlling-the-sensitivity-of-support-vector-machines-cristianini2002",
    "href": "Journal/eric.html#controlling-the-sensitivity-of-support-vector-machines-cristianini2002",
    "title": "Eric’s Journal Entries",
    "section": "",
    "text": "https://seis.bristol.ac.uk/~enicgc/pubs/1999/ijcai_ss.pdf\nThe above speaks to controlling the sensitivity of Support Vector Machines to reduce the number of False Positives/Negatives in the output. Veropoulos, Campbell, and Cristianini go on to detail the difference between Sensitivity and Specificity through various mathematical approaches and analyses. Their research with medical data sets where Box constraints were not statistically significant between the 4 data sets. However, the strain on the algorithm was insignificant and could aid in over all determinations."
  },
  {
    "objectID": "Journal/eric.html#time-series-prediction-using-support-vector-machines-a-survey-sapankevych2009",
    "href": "Journal/eric.html#time-series-prediction-using-support-vector-machines-a-survey-sapankevych2009",
    "title": "Eric’s Journal Entries",
    "section": "Time Series Prediction Using Support Vector Machines: A Survey (Sapankevych and Sankar 2009)",
    "text": "Time Series Prediction Using Support Vector Machines: A Survey (Sapankevych and Sankar 2009)\nhttps://ieeexplore.ieee.org/abstract/document/4840324\nThis article details the ways in which Support Vector Machines have been utilized to perform time series analysis. I found this very interesting as in my current line of work we are looking for ways to implement more Machine Learning into our models and we do a fair amount of Time Series Analysis. This is a important topic for us as we start to work through the best way we can utilize and implement our model and how to best identify the uses for the model. Additionally this article delves into SVR and how that methodology can also be utilized in Time Series modeling."
  },
  {
    "objectID": "Journal/eric.html#a-comparative-analysis-of-k-nearest-neighbor-genetic-support-vector-machine-decision-tree-and-long-short-term-memory-algorithms-in-machine-learning-bansal2022",
    "href": "Journal/eric.html#a-comparative-analysis-of-k-nearest-neighbor-genetic-support-vector-machine-decision-tree-and-long-short-term-memory-algorithms-in-machine-learning-bansal2022",
    "title": "Eric’s Journal Entries",
    "section": "A comparative analysis of K-Nearest Neighbor, Genetic, Support Vector Machine, Decision Tree, and Long Short Term Memory algorithms in machine learning (Bansal, Goyal, and Choudhary 2022)",
    "text": "A comparative analysis of K-Nearest Neighbor, Genetic, Support Vector Machine, Decision Tree, and Long Short Term Memory algorithms in machine learning (Bansal, Goyal, and Choudhary 2022)\nhttps://www.sciencedirect.com/science/article/pii/S2772662222000261\nThe above article delves into the comparative difference between a few different methodologies relative to Machine Learning, such as KNN, SVM, and LSTM. The article further details the strengths and weaknesses of each and their most practical use cases based on what the implementer is ultimately seeking to achieve. Ultimately this provides more context on how to best utilize our model and to most effectively choose our groups data set so that when we start building our project we are not starting off on the wrong foot."
  },
  {
    "objectID": "Journal/eric.html#fast-training-support-vector-machines-using-parallel-sequential-minimal-optimization-zeng2008",
    "href": "Journal/eric.html#fast-training-support-vector-machines-using-parallel-sequential-minimal-optimization-zeng2008",
    "title": "Eric’s Journal Entries",
    "section": "Fast training Support Vector Machines using parallel sequential minimal optimization (Zeng et al. 2008)",
    "text": "Fast training Support Vector Machines using parallel sequential minimal optimization (Zeng et al. 2008)\nhttps://ieeexplore.ieee.org/abstract/document/4731075\nThe article above details out the various ways we can quickly train SVM models utilizing Sequential Minimal Optimization (SMO) algorithms to reduce the problems that can arise from large scale programming. A parallel SMO method was the primary focus of this paper as it covered the basic functions and algorithms behind the inner workings as applied to an SVM."
  },
  {
    "objectID": "Journal/eric.html#support-vector-machine-accuracy-improvement-with-classification-mohan2020",
    "href": "Journal/eric.html#support-vector-machine-accuracy-improvement-with-classification-mohan2020",
    "title": "Eric’s Journal Entries",
    "section": "Support Vector Machine Accuracy Improvement with Classification (Mohan et al. 2020)",
    "text": "Support Vector Machine Accuracy Improvement with Classification (Mohan et al. 2020)\nhttps://ieeexplore.ieee.org/abstract/document/9242572\nThis article walks through how to successfully setup and run an SVM based on a binary classification problem. Furthermore, it details the inner workings of various types of kernels that can be used to accurately map your planes based on the complexity of the data. I found this particularly interesting as I was wondering about what kernel to build upon and it seems as if an RBF or a Gaussian kernel might be our ticket."
  },
  {
    "objectID": "Journal/eric.html#effectiveness-of-random-search-in-svm-hyper-parameter-tuning-mantovani2015",
    "href": "Journal/eric.html#effectiveness-of-random-search-in-svm-hyper-parameter-tuning-mantovani2015",
    "title": "Eric’s Journal Entries",
    "section": "Effectiveness of Random Search in SVM hyper-parameter tuning (Mantovani et al. 2015)",
    "text": "Effectiveness of Random Search in SVM hyper-parameter tuning (Mantovani et al. 2015)\nhttps://ieeexplore.ieee.org/abstract/document/7280664\nThe authors of this article delve into the theory and techniques of tuning the Hyper parameters utilized by the SVM in relation classification, specifically Random Search. Often times this tuning requires a significant time investment with trial and error. It can also require specific knowledge of the data and outcomes associated with the training of the model, and frequently different variables will yield greater results dependent on the outcome. They concluded that given low dimensionality data sets Random Search can yield similar predictive results as more complex tuning methods such as grid search and meta-heuris tics."
  },
  {
    "objectID": "Journal/eric.html#empirical-evaluation-of-resampling-procedures-for-optimising-svm-hyperparameters",
    "href": "Journal/eric.html#empirical-evaluation-of-resampling-procedures-for-optimising-svm-hyperparameters",
    "title": "Eric’s Journal Entries",
    "section": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
    "text": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters\nhttps://www.jmlr.org/papers/volume18/16-174/16-174.pdf\nThe paper delves into the Radial Basis Function (RBF) Kernel of SVM’s and discusses the need for hyperparameter selection between a regularization and a governing parameter. The paper goes on to detail that there is no generally accepted means for the optimization of hyperparameters. Once approach is to continuously re-sample the initial training data from the data set utilizing different methods and then evaluate them. Ultimately the paper concludes that based on the research and testing of 17 varying methods of 121 data sets there is no definite solution to determine a one size fits all approach to selection of parameters based on an expected outcome."
  },
  {
    "objectID": "Journal/brad.html",
    "href": "Journal/brad.html",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient's return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.\" Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.\" Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.\" Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\nIntroduction\nSupport Vector Machines (SVM) are a great way to mine data in Electronic Medical Records (EMR). You can use them to find patterns in the data that might be hard to find with regular statistical methods. SVMs can also be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before you can use SVMs for data mining in EMRs, you need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. Here are some more reasons why using SVMs for data mining in EMRs is helpful in the clinical setting.  SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. Also, models that are easy to understand can be made with SVMs which is important to medical providers so they can explain it to patients and their family.  This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs are very helpful tools for mining data in EMRs since you get useful information from the data to make models that can improve the care of patients. This can assist in predicting those patients at risk for mortality or death in the Intensive Care Unit (ICU) which are the sickest of the patients."
  },
  {
    "objectID": "Journal/brad.html#summary-of-articles",
    "href": "Journal/brad.html#summary-of-articles",
    "title": "SVM application in Data Mining in EMR",
    "section": "",
    "text": "Support vector machines (SVMs) are a powerful method for machine learning that can be used for data mining. There are several different SVM kernels, and it is not always clear which one is best for a certain job. The goal of this paper is to help data scientists pick the best SVM kernel for a given job. The authors looked at how well different SVM models did at classification, regression, and clustering, among other data mining tasks. They used both real-world data and data that they made up themselves. The article by Xu et al. aimed to see how well various SVM kernels did at data mining jobs. They found that SVM with the RBF kernel did the best job at most data mining tasks. However, they also found that the performance of the different SVM kernels relies on the task and data set. One problem with this study is that there were only a few data mining jobs carried out. (5)\nMy next journal suggested a new SVM algorithm for jobs related to data mining. This is important since SVM is a powerful machine learning method, but they can be hard to train, especially on big datasets. The goal of this study is to suggest a new SVM algorithm that works better for data mining. They came up with a new SV algorithm that is made for data mining jobs. The program uses several methods to improve how well SVM training works. They tested how well their new SVM algorithm did at classification, regression, and grouping, among other tasks in data mining. They found that their new SVM algorithm was better at most data mining jobs than other SVM algorithms. But this algorithm has a weakness in that it is harder to understand than other SVM algorithms. (3)\nIn addition, Zhou et al wrote about deep mining of electronic medical data using support vector machines to predict the prognosis of severe, acute myocardial infarction. The authors talked about how the MIMIC-3 database is used to find the 13 markers for heart attack cases. They compared SVM algorithms and found that the model was about 92% accurate. They use this model to pull out certain features from the EMR and identify which patients will have a MI. They said that this helps doctors figure out the classification regression parts of a disease outlook. (6)\nMy next piece was about how Fouodo et al and others used support vector machines for survival analysis with R. They used the survivalSVM package to do three different kinds of survival analysis. They used both regression and ranking, which is a mix of the two. The next way to find the constraints was to use regression followed by Cox proportional hazard models. They stated that the SVM worked about as well as other methods on the datasets they used. So, this R package makes it quick and easy to find out how likely a patient is to live. (2)\nAnother article was called “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records.” The goal of this work is to show how support vector machines (SVMs) in electronic medical records (EMRs) can be used to classify diabetes mellitus. This study looked at how well SVMs can classify diabetes because they have been good at diagnosing other diseases from electronic medical records (EMRs). The writers used EMRs from both people with and without diabetes to train an SVM model. During preparation, noise and outliers were first taken out of the EMRs. The SVM model was then trained with the help of guided learning. (1)\nThe next journal discussed a way to predict hospital readmissions using support vector machines. The goal of this study is to make a support vector machine (SVM) model that can predict a patient's return to the hospital. The importance was that going back to the hospital is a deadly problem in health care, and it can be expensive for patients. A reliable predictor of hospital readmission could help hospitals find people who are at risk and give them treatment to keep them from going back to the hospital. A solution is that a collection of electronic medical records (EMRs) was used to train an SVM model. First, during preprocessing, abnormalities were taken out of the EMRs. The SVM model was then trained with the help of guided learning and separated the information into two groups. With the SVM model, this included readmitted patients who had to go back to the hospital. (4)\nSVM was used by Vieira et al. to divide data into two groups. The algorithm maps the raw data to a high-dimensional feature space, where a linear classification surface is made. The SVM method then tries to find the best hyperplane that separates the two types of data by the most. The margin is the distance between the hyperplane and the data points in each group that are closest to the hyperplane. The SVM algorithm also uses a kernel function to move the raw data into a space with more dimensions, where it is easier to separate. The kernel function is a piece of math that figures out how similar two data points are to each other. SVM also uses regularization to control the trade-off between making the margin as big as possible and making the classification mistake as small as possible. The SVM algorithm learns from a set of labeled data, where each data point has a label that tells what group it belongs to. Once the SVM algorithm has been taught, it can be used to put new data points that have not been labeled into one of the two groups. (7)\nYang et al. evaluated the performance a version of GAN called conditional medical GAN (C-med GAN) could determine who would die among ICU patients. The study used data from the Medical Information Mart for Intensive Care III (MIMIC-III) database and compared the success of the C-med GAN with some baseline models, such as the simplified acute physiology score II (SAPS II), the support vector machine (SVM), and the multilayer perceptron (MLP). The dataset was split into three sizes, and a 5-fold grid search cross-validation process was used to find the best hyperparameters and then the best model selection for the C-med GAN. Area under the precision-recall curve (PR-AUC), area under the receiver operating characteristic curve (ROC-AUC), and F1 score were used to measure the C-med GAN’s accuracy. The study came up with a helpful method to use SAPS II results to directly estimate how long a patient will live. The results of this study could be used in intensive care to make it easier to predict mortality in the ICU. (8)\nReferences\n(1) Adeoye, Abiodun O., et al. Utilizing Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records. International Journal of Advanced Computer Science and Information Technology (IJACSIT), vol. 11, no. 10, 2021, pp. 102-114.\n(2) Fouodo, Cesaire, et al. Support Vector Machines for Survival Analysis with R. R Journal, vol. 14, no. 2, 2022, pp. 92-107.\n(3) Hu, Xiangfen, Wei Huang, and Qiang Wu. A New Support Vector Machine Algorithm for Data Mining.\" Knowledge-Based Systems, vol. 112, 2016, pp. 118-128.\n(4) Ismail, Gaber A., et al. An Approach Using Support Vector Machines to Predict Hospital Readmission.\" Journal of Medical Systems, vol. 44, no. 9, 2020, pp. 1-10.\n(5) Xu, Fei, Lihong Li, and Zhihua Zhou. SVM Kernels for Data Mining: A Comparative Study.\" Proceedings of the 2010 SIAM International Conference on Data Mining (SDM), 2010, 585-596.\n(6) Zhou, Xingyu, et al. Using Support Vector Machines for Deep Mining of Electronic Medical Records in Order to Predict Prognosis of Severe, Acute Myocardial Infarction. Frontiers in Cardiovascular Medicine, vol. 10, 2023, p.918.\n(7) Vieira, S.M., Mendonça, L. F., Farinha, G. J., & Sousa, J. M. C. (2013). Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients. Applied Soft Computing, 13(8), 3494–3504. https://doi.org/10.1016/j.asoc.2013.03.021\n(8) Yang, Zou, H., Wang, M., Zhang, Q., Li, S., & Liang, H. (2023). Mortality prediction among ICU inpatients based on MIMIC-III database results from the conditional medical generative adversarial network. Heliyon, 9(2), e13200–e13200. https://doi.org/10.1016/j.heliyon.2023.e13200\nIntroduction\nSupport Vector Machines (SVM) are a great way to mine data in Electronic Medical Records (EMR). You can use them to find patterns in the data that might be hard to find with regular statistical methods. SVMs can also be used to make models that can use new data to make accurate predictions. It is important to keep in mind, though, that SVMs can be hard to train, especially on big datasets. SVMs can also be responsive to how the SVM kernel and hyperparameters are chosen. Once the SVM model has been trained, it can be used to guess what will happen with new data. For example, a model could be used to figure out how likely it is that a patient will get a certain illness or what will happen to a patient who already has that disease. Before you can use SVMs for data mining in EMRs, you need to prepare the data since the noise and outliers should be taken out of the data. It is also important to feature engineer the data, which will help make new features that may be more useful for the SVM model.\nSVMs can be used to get useful information from the data and to make models that can improve the care of patients. Here are some more reasons why using SVMs for data mining in EMRs is helpful in the clinical setting.  SVMs can determine a model to predict which patients might survive a severe illness in the hospital. This is important for data mining in EMRs, where the data is often complicated since SVMs can handle noise and errors well. This is important for data mining in EMRs because the data may have errors or missing data. Also, models that are easy to understand can be made with SVMs which is important to medical providers so they can explain it to patients and their family.  This is important for data mining in EMRs because it would be helpful to know how the models work, so we can have evidence to support the results. Overall, SVMs are very helpful tools for mining data in EMRs since you get useful information from the data to make models that can improve the care of patients. This can assist in predicting those patients at risk for mortality or death in the Intensive Care Unit (ICU) which are the sickest of the patients."
  },
  {
    "objectID": "Journal/brad.html#methods",
    "href": "Journal/brad.html#methods",
    "title": "SVM application in Data Mining in EMR",
    "section": "Methods",
    "text": "Methods"
  },
  {
    "objectID": "Journal/brad.html#analysis-and-results",
    "href": "Journal/brad.html#analysis-and-results",
    "title": "SVM application in Data Mining in EMR",
    "section": "Analysis and Results",
    "text": "Analysis and Results\n\nData and Visualisation\nA study was conducted to determine how…\n\n\nCode\n# loading packages \nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggthemes)\nlibrary(ggrepel)\nlibrary(dslabs)\n\n\n\n\nCode\n# Load Data\nkable(head(murders))\n\n\n\n\n\nstate\nabb\nregion\npopulation\ntotal\n\n\n\n\nAlabama\nAL\nSouth\n4779736\n135\n\n\nAlaska\nAK\nWest\n710231\n19\n\n\nArizona\nAZ\nWest\n6392017\n232\n\n\nArkansas\nAR\nSouth\n2915918\n93\n\n\nCalifornia\nCA\nWest\n37253956\n1257\n\n\nColorado\nCO\nWest\n5029196\n65\n\n\n\n\n\nCode\nggplot1 = murders %&gt;% ggplot(mapping = aes(x=population/10^6, y=total)) \n\n  ggplot1 + geom_point(aes(col=region), size = 4) +\n  geom_text_repel(aes(label=abb)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  geom_smooth(formula = \"y~x\", method=lm,se = F)+\n  xlab(\"Populations in millions (log10 scale)\") + \n  ylab(\"Total number of murders (log10 scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name = \"Region\")+\n      theme_bw()\n\n\n\n\n\n\n\nStatistical Modeling\n\n\nConclusion"
  },
  {
    "objectID": "Journal/brad.html#references",
    "href": "Journal/brad.html#references",
    "title": "SVM application in Data Mining in EMR",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "",
    "text": "Every day sensors and systems are capturing a virtual flood of data points and feeding those values into powerful artificial intelligence and machine learning systems to derive classifications and predict a myriad of outcomes. These systems help us do a broad spectrum of things from fraud detection to interactions with smart home systems. Given the number of data points that are present in the medical field, it is of no surprise that machine learning is increasingly being leveraged to elevate patient care.\nFor as long as most of us remember, our interactions with physicians have included the gathering of data points to help them detect illness and track progression of disease. Common data points are age, weight, height, temperature, blood pressure, list of any current symptoms and etc. Physicians then use their education and years of practice to provide diagnoses and help us live happy and healthy lives. But what if this was process could be supported with machine learning, and brought into a more critical care setting?\nWe can. Thanks to machine learning we can use data from the countless sensors and measurements taken by medical staff in Intensive Care Units (ICU) to predict the survivability of patients under care. This data can then help teams organize around certain cases to help ensure the best allotment of resources and highest attention to the most dire of cases. A method we will use in this report is through the construction of Support Vector Machines or SVM’s.\nSVM’s are a machine learning methodology that uses supervised learning based on historic cases to help train models that can then be used on other cases. Models created by Support Vector Machines are used to create clusters of 2 distinct groups of data based on the creation of a maximally-marginal hyperplane.(Han and Pei 2012). A maximally-marginal hyperplane can be explained as a line that can be drawn between two clusters that separates their members with the greatest distance between members of each cluster that are their nearest.\nWhile we did not find many cases of using support vector machines for ICU patient survival, the use of Support Vector Machines in medicine is not a novel approach. In “Using Support Vector Machines for Diabetes Mellitus Classification from Electronic Medical Records” by Adeoye the author leveraged electronic medical records to help classify individuals with and without diabetes. Meanwhile, Zhou et al where able to use Support Vector Machines to predict the prognosis of severe, acute myocardial infarction with 92% accuracy from electronic medical records(Zhou 2023)\nWhile Support Vector Machines can be a great utility used to cluster and predict outcomes, their usage in Medicine is not without problems. One problem noted by Liu et al is that the sheer number of data points available can make it hard to find features (data points of importance) for use in model construction.(Liu June 2018) In fact Liu goes as far as describing the use of Principal Component Analysis to chose which features to include in model construction. Additionally, support vector machines work best when the data can be mapped linearly, however unlike other methodologies this is not a strict requirement. Should data not be easily linearly separable the use of a kernel function or kernel trick allows data to be mapped from one space to another for optimal construction of the maximally marginal hyperplane (Han and Pei 2012) (Mohan et al. 2020)"
  },
  {
    "objectID": "index.html#data-and-vizualisation",
    "href": "index.html#data-and-vizualisation",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Data and Vizualisation",
    "text": "Data and Vizualisation"
  },
  {
    "objectID": "index.html#statistical-modeling",
    "href": "index.html#statistical-modeling",
    "title": "Predicting Survival of Intensive Care Unit Patients with Support Vector Machines",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling"
  }
]